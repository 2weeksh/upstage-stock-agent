from langchain_upstage import ChatUpstage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from app.utils.llm import get_solar_model


class BaseAgent:
    def __init__(self, name, role, retriever, category):
        self.name = name
        self.role = role
        self.retriever = retriever # get_vector_db(ticker)ë¡œ ë°˜í™˜ëœ Chroma ê°ì²´
        self.llm = get_solar_model() # ì—…ìŠ¤í…Œì´ì§€ì˜ ìµœì‹  ëª¨ë¸ ì‚¬ìš©
        self.parser = StrOutputParser()
        self.category = category        # ë³¸ì¸ì˜ ì „ê³µ ì¹´í…Œê³ ë¦¬ (news, chart, finance ë“±)

        self.context_cache = {}

    def _get_context(self, query, category=None, debug=False):
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ì§€ì‹ì„ ë²¡í„° DBì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤. (ìºì‹œ ì ìš©)
        """

        # 1. ìºì‹œ í™•ì¸: í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì§€ì‹ì„ ì´ë¯¸ ê°€ì ¸ì™”ë‹¤ë©´ ê²€ìƒ‰ ìƒëµ
        if category in self.context_cache:
            return self.context_cache[category]

        # 1. ì¹´í…Œê³ ë¦¬ í•„í„° ì„¤ì •
        search_filter = {"category": category} if category else None

        # # kê°’ì€ ë°ì´í„°ì˜ ì¤‘ìš”ë„ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥ (DARTëŠ” ì¡°ê¸ˆ ë” ë§ì´ ê°€ì ¸ì˜´)
        k_value = 4 if category == "common" else 3

        # 2. ë²¡í„° DB ê²€ìƒ‰ (similarity_search ì‚¬ìš©)
        docs = self.retriever.similarity_search(
            query, 
            k=k_value, 
            filter=search_filter
        )

        # 3. ìºì‹œì— ì €ì¥
        context_str = "\n\n".join([doc.page_content for doc in docs])
        self.context_cache[category] = context_str
        
        # [ë””ë²„ê·¸] ê²€ìƒ‰ëœ ì •ë³´ í™•ì¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if debug:
            print(f"\nğŸ” [{self.name}] RAG ê²€ìƒ‰ ìˆ˜í–‰ (Category: {category})")
            print(f"   > ì¿¼ë¦¬: {query}")
            for i, doc in enumerate(docs):
                source = doc.metadata.get('source', 'Unknown')
                # ë‚´ìš©ì˜ ì²« 100ì ë¯¸ë¦¬ë³´ê¸°
                content_preview = doc.page_content.replace('\n', ' ')[:100]
                print(f"     [{i+1}] {source}: {content_preview}...")
            print("="*30)

        return context_str

    def _get_dual_context(self, query, debug=False):
        """
        [ì´ì¤‘ ê²€ìƒ‰ í•µì‹¬ ë¡œì§]
        1. 'common'(DART)ì—ì„œ ê³µì‹ì ì¸ ê¸°ì—… ê¸°ë³¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        2. 'self.category'(ì „ê³µ)ì—ì„œ ì—ì´ì „íŠ¸ íŠ¹í™” ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        """
        # 1. ê³µí†µ ì§€ì‹ í™•ë³´ (DART)
        common_context = self._get_context(query, category="common", debug=debug)
        
        # 2. ì „ê³µ ì§€ì‹ í™•ë³´ (news, chart, finance ì¤‘ í•˜ë‚˜)
        special_context = self._get_context(query, category=self.category, debug=debug)

        # 3. ë‘ ì •ë³´ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ê²°í•©
        combined_context = f"""
### [1. ê³µì‹ ë³´ê³ ì„œ ê¸°ë°˜ ê¸°ì´ˆ ë°ì´í„° (DART)]
{common_context}

### [2. ìµœì‹  {self.category.upper()} íŠ¹í™” ë°ì´í„°]
{special_context}
"""
        return combined_context

    def create_prompt(self, context, query):
        """ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•  í”„ë¡¬í”„íŠ¸ ìƒì„± ì¶”ìƒ ë©”ì„œë“œ"""
        raise NotImplementedError("ìì‹ í´ë˜ìŠ¤ì—ì„œ create_promptë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")


    def analyze(self, company_name, ticker, debate_context=None, debug=False):
        """
        ì—ì´ì „íŠ¸ê°€ ì´ì¤‘ ê²€ìƒ‰ëœ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # 1. ì´ì¤‘ ê²€ìƒ‰ ìˆ˜í–‰
        search_query = f"{company_name} {ticker} ì¬ë¬´ ì‹¤ì  í˜„í™© ì´ìŠˆ ë¶„ì„"
        context = self._get_dual_context(search_query, debug=debug)

        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„± (í† ë¡  ë§¥ë½ì´ ìˆë‹¤ë©´ í¬í•¨)
        query_text = f"{company_name}({ticker})ì— ëŒ€í•œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”."
        if debate_context:
            query_text += f"\n\n[ì´ì „ í† ë¡  ë§¥ë½]\n{debate_context}"

        prompt = self.create_prompt(context, query_text)
        
        # 3. LLM í˜¸ì¶œ
        response = self.llm.invoke(prompt)
        return response.content