"""
ì‹¤ì œ ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ vs ë‹¨ì¼ ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¹„êµ

[ë¹„êµ ëŒ€ìƒ]
- ë©€í‹°: ì‹¤ì œ StockService (ê¸°ì¡°ë°œì–¸ â†’ í† ë¡  â†’ ìµœí›„ë³€ë¡  â†’ ì‚¬íšŒììš”ì•½ â†’ JudgeíŒì •)
- ë‹¨ì¼: SingleAgentAnalyzer (ëª¨ë“  ë°ì´í„° í•œ ë²ˆì— ë¶„ì„)

ì‹¤í–‰ ë°©ë²•:
    python evaluate/compare_with_debate.py

ê²°ê³¼:
    - evaluate/results/debate_comparison_YYYYMMDD_HHMMSS.json
    - evaluate/results/debate_comparison_YYYYMMDD_HHMMSS_summary.md
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import asyncio
import re
from typing import Dict, List
from datetime import datetime
from pathlib import Path

from app.utils.llm import get_solar_model
from app.tools.chart_tools import get_chart_indicators
from app.tools.finance_tools import get_financial_summary
from app.tools.search_tools import get_stock_news
from app.utils.ticker_utils import get_clean_ticker


class SingleAgentAnalyzer:
    """
    ë‹¨ì¼ ì—ì´ì „íŠ¸: ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë°›ì•„ì„œ ë¶„ì„
    (ê¸°ì¡´ê³¼ ë™ì¼ - í† ë¡  ì—†ìŒ)
    """
    
    def __init__(self):
        self.llm = get_solar_model(temperature=0.2)
    
    def analyze(self, company_name: str, ticker: str, 
                chart_data: str, finance_data: str, news_data: str) -> Dict:
        
        prompt = f"""
        ë‹¹ì‹ ì€ ì¢…í•© íˆ¬ì ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ {company_name}({ticker})ì— ëŒ€í•œ íˆ¬ì ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

        [ì°¨íŠ¸ ë¶„ì„ ë°ì´í„°]
        {chart_data}

        [ì¬ë¬´ ë¶„ì„ ë°ì´í„°]
        {finance_data}

        [ë‰´ìŠ¤ ë° ì‹œì¥ ì‹¬ë¦¬ ë°ì´í„°]
        {news_data}

        ---
        ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

        ### 1. ìµœì¢… íˆ¬ì ë“±ê¸‰
        [ê°•ë ¥ ë§¤ìˆ˜ / ë§¤ìˆ˜ / ì¤‘ë¦½ / ë§¤ë„ / ê°•ë ¥ ë§¤ë„]

        ### 2. ì ìˆ˜
        [0.0 ~ 10.0 ì‚¬ì´ì˜ ì ìˆ˜]

        ### 3. í•µì‹¬ íˆ¬ì ë…¼ë¦¬ (3ê°€ì§€)
        1. [ë…¼ë¦¬ 1]
        2. [ë…¼ë¦¬ 2]
        3. [ë…¼ë¦¬ 3]

        ### 4. ì£¼ìš” ë¦¬ìŠ¤í¬ (3ê°€ì§€)
        1. [ë¦¬ìŠ¤í¬ 1]
        2. [ë¦¬ìŠ¤í¬ 2]
        3. [ë¦¬ìŠ¤í¬ 3]

        ### 5. íŠ¸ë ˆì´ë”© ì „ëµ
        - ì ì • ì§„ì…ê°€: [êµ¬ì²´ì  ê°€ê²©]
        - 1ì°¨ ëª©í‘œê°€: [ê°€ê²©]
        - ì†ì ˆê°€: [ê°€ê²©]
        """
        
        response = self.llm.invoke(prompt).content
        
        return {
            "type": "single_agent",
            "company": company_name,
            "ticker": ticker,
            "analysis": response,
            "timestamp": datetime.now().isoformat()
        }


class RealMultiAgentSystem:
    """
    ì‹¤ì œ ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ
    (StockServiceì˜ ì „ì²´ í”„ë¡œì„¸ìŠ¤ í™œìš©)
    """
    
    def __init__(self):
        from app.service.stock_service import StockService
        self.stock_service = StockService()
    
    async def analyze(self, company_name: str) -> Dict:
        """
        ì‹¤ì œ í† ë¡  ì‹œìŠ¤í…œ ì‹¤í–‰
        
        í”„ë¡œì„¸ìŠ¤:
        1. ê¸°ì¡° ë°œì–¸ (ì°¨íŠ¸/ì¬ë¬´/ë‰´ìŠ¤ ì—ì´ì „íŠ¸)
        2. ìƒí˜¸ í† ë¡  (ìµœëŒ€ 10ë¼ìš´ë“œ, ì‚¬íšŒì ì£¼ë„)
        3. ìµœí›„ ë³€ë¡ 
        4. ì‚¬íšŒì ìš”ì•½
        5. Judge ìµœì¢… íŒì •
        6. Report ìƒì„±
        """
        
        user_question = f"{company_name} ë¶„ì„í•´ì¤˜"
        
        # ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ìˆ˜ì§‘
        discussion_log = []
        final_summary = None
        final_conclusion = None
        max_debate_round = 0  # ì‹¤ì œ í† ë¡  ë¼ìš´ë“œ ì¶”ì 
        
        print(f"    ğŸ”„ ì‹¤ì œ í† ë¡  ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘... (ì•½ 2-3ë¶„ ì†Œìš”)")
        
        try:
            async for event_str in self.stock_service.handle_user_task(user_question):
                try:
                    event_data = json.loads(event_str)
                    event_type = event_data.get('type')
                    
                    # status ë©”ì‹œì§€ì—ì„œ ì‹¤ì œ í† ë¡  ë¼ìš´ë“œ ë²ˆí˜¸ ì¶”ì¶œ
                    if event_type == 'status':
                        message = event_data.get('message', '')
                        # "ìƒí˜¸ í† ë¡  5/10 ë¼ìš´ë“œ" íŒ¨í„´ì—ì„œ ì¶”ì¶œ
                        match = re.search(r'ìƒí˜¸ í† ë¡  (\d+)/\d+ ë¼ìš´ë“œ', message)
                        if match:
                            round_num = int(match.group(1))
                            max_debate_round = max(max_debate_round, round_num)
                    
                    # í† ë¡  ê³¼ì • ê¸°ë¡ (ì „ì²´ discussion_logëŠ” ë¶„ì„ìš©ìœ¼ë¡œ ìœ ì§€)
                    if event_type == 'debate':
                        speaker = event_data.get('speaker', 'unknown')
                        message = event_data.get('message', '')
                        discussion_log.append({
                            'speaker': speaker,
                            'message': message
                        })
                    
                    # ìµœì¢… ê²°ê³¼ ìˆ˜ì§‘
                    elif event_type == 'result':
                        result_data = event_data.get('data', {})
                        final_summary = result_data.get('summary', '')
                        final_conclusion = result_data.get('conclusion', '')
                        break  # ìµœì¢… ê²°ê³¼ ë°›ìœ¼ë©´ ì¢…ë£Œ
                        
                except json.JSONDecodeError:
                    continue  # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
        
        except Exception as e:
            print(f"    âŒ í† ë¡  ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "type": "multi_agent_with_debate",
                "company": company_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        
        return {
            "type": "multi_agent_with_debate",
            "company": company_name,
            "discussion_log": discussion_log,
            "summary": final_summary,
            "conclusion": final_conclusion,
            "debate_rounds": max_debate_round,  # ì‹¤ì œ í† ë¡  ë¼ìš´ë“œ (ìµœëŒ€ 10)
            "timestamp": datetime.now().isoformat()
        }


class DebateComparator:
    """ì‹¤ì œ í† ë¡  ì‹œìŠ¤í…œ vs ë‹¨ì¼ ì—ì´ì „íŠ¸ ë¹„êµ í‰ê°€"""
    
    def __init__(self):
        self.single_agent = SingleAgentAnalyzer()
        self.multi_agent = RealMultiAgentSystem()
        self.judge_llm = get_solar_model(temperature=0.0)  # í‰ê°€ëŠ” ì¼ê´€ì„± ì¤‘ìš”
    
    async def run_comparison(self, test_stocks: List[str]) -> Dict:
        """
        ë¹„êµ í‰ê°€ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            test_stocks: ["ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤", ...] (í•œê¸€ ê¸°ì—…ëª…)
        """
        results = {
            "metadata": {
                "test_date": datetime.now().isoformat(),
                "total_stocks": len(test_stocks),
                "evaluator": "LLM-as-Judge (Solar Pro 2)",
                "multi_agent_type": "Full Debate System (ê¸°ì¡°ë°œì–¸ + í† ë¡  + ìµœí›„ë³€ë¡  + ì‚¬íšŒììš”ì•½ + íŒì •)"
            },
            "stocks": []
        }
        
        for company_name in test_stocks:
            print(f"\n{'='*70}")
            print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì¢…ëª©: {company_name}")
            print(f"{'='*70}\n")
            
            try:
                # 1. í‹°ì»¤ ì¶”ì¶œ
                ticker = get_clean_ticker(company_name)
                print(f"  âœ… í‹°ì»¤: {ticker}")
                
                # 2. ë°ì´í„° ìˆ˜ì§‘ (ë‹¨ì¼ ì—ì´ì „íŠ¸ìš©)
                print(f"  ğŸ“¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                chart_data = get_chart_indicators(ticker)
                finance_data = get_financial_summary(ticker)
                news_data = get_stock_news(ticker, company_name)
                
                # 3. ë©€í‹° ì—ì´ì „íŠ¸ ë¶„ì„ (ì‹¤ì œ í† ë¡  ì‹œìŠ¤í…œ)
                print(f"  ğŸ­ ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ ì‹¤í–‰...")
                multi_result = await self.multi_agent.analyze(company_name)
                
                if "error" in multi_result:
                    print(f"  âŒ ë©€í‹° ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {multi_result['error']}")
                    results["stocks"].append({
                        "company": company_name,
                        "ticker": ticker,
                        "error": multi_result["error"]
                    })
                    continue
                
                print(f"  âœ… í† ë¡  ì™„ë£Œ (ë¼ìš´ë“œ ìˆ˜: {multi_result.get('debate_rounds', 0)})")
                
                # 4. ë‹¨ì¼ ì—ì´ì „íŠ¸ ë¶„ì„
                print(f"  ğŸ”„ ë‹¨ì¼ ì—ì´ì „íŠ¸ ë¶„ì„ ì‹¤í–‰...")
                single_result = self.single_agent.analyze(
                    company_name, ticker, chart_data, finance_data, news_data
                )
                print(f"  âœ… ë‹¨ì¼ ì—ì´ì „íŠ¸ ë¶„ì„ ì™„ë£Œ")
                
                # 5. í‰ê°€
                print(f"  âš–ï¸  LLM-as-Judge í‰ê°€ ì¤‘...")
                evaluation = await self._evaluate_pair(
                    company_name, multi_result, single_result
                )
                print(f"  âœ… í‰ê°€ ì™„ë£Œ")
                
                # 6. ê²°ê³¼ ì €ì¥
                results["stocks"].append({
                    "company": company_name,
                    "ticker": ticker,
                    "multi_agent_result": multi_result,
                    "single_agent_result": single_result,
                    "evaluation": evaluation
                })
                
                print(f"  âœ… {company_name} ì „ì²´ í‰ê°€ ì™„ë£Œ\n")
                
            except Exception as e:
                import traceback
                print(f"  âŒ {company_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                traceback.print_exc()
                results["stocks"].append({
                    "company": company_name,
                    "error": str(e)
                })
        
        # 7. ì¢…í•© ìš”ì•½
        results["summary"] = self._generate_summary(results["stocks"])
        
        return results
    
    async def _evaluate_pair(self, company_name: str, 
                            multi_result: Dict, single_result: Dict) -> Dict:
        """LLM-as-Judge í‰ê°€"""
        
        evaluation_prompt = f"""
        ë‹¹ì‹ ì€ ê°ê´€ì ì¸ íˆ¬ì ë¶„ì„ í‰ê°€ìì…ë‹ˆë‹¤.
        ë‘ AI ì‹œìŠ¤í…œì´ {company_name}ë¥¼ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ 5ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

        [ì‹œìŠ¤í…œ A - ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ]
        - 3ëª…ì˜ ì „ë¬¸ê°€(ì°¨íŠ¸/ì¬ë¬´/ë‰´ìŠ¤)ê°€ ê¸°ì¡° ë°œì–¸
        - ì‚¬íšŒì ì£¼ë„ë¡œ ìƒí˜¸ í† ë¡  ì§„í–‰ ({multi_result.get('debate_rounds', 0)}ë¼ìš´ë“œ)
        - ë°˜ë°•ì„ í†µí•œ ë¦¬ìŠ¤í¬ ë°œê²¬ ë° ë…¼ë¦¬ ê²€ì¦
        - ìµœí›„ ë³€ë¡  ë° ì‚¬íšŒì ìš”ì•½
        - Judgeì˜ ìµœì¢… íŒì •

        ìµœì¢… ê²°ë¡ :
        {multi_result.get('conclusion', '')}

        [ì‹œìŠ¤í…œ B - ë‹¨ì¼ ì—ì´ì „íŠ¸ (í†µí•© ë¶„ì„)]
        - í•œ ëª…ì˜ AIê°€ ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë¶„ì„
        - í† ë¡  ê³¼ì • ì—†ìŒ

        ê²°ê³¼:
        {single_result.get('analysis', '')}

        ---
        **í‰ê°€ ê¸°ì¤€ (ê° 1~10ì )**

        1. **ë…¼ë¦¬ì  íƒ€ë‹¹ì„± (Logical Coherence)**
        - ë¶„ì„ ê·¼ê±°ê°€ ëª…í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ìˆëŠ”ê°€?

        2. **ë‹¤ê°ì  ê´€ì  (Multi-Perspective)** â­ í•µì‹¬!
        - ê¸°ìˆ ì /ì¬ë¬´ì /ì‹¬ë¦¬ì  ì¸¡ë©´ì„ ê· í˜•ìˆê²Œ ê³ ë ¤í–ˆëŠ”ê°€?
        - í† ë¡ ì„ í†µí•´ ê° ê´€ì ì´ ì‹¬í™”ë˜ì—ˆëŠ”ê°€?

        3. **ë¦¬ìŠ¤í¬ ì¸ì‹ (Risk Awareness)** â­ í•µì‹¬!
        - íˆ¬ì ë¦¬ìŠ¤í¬ë¥¼ ì¶©ë¶„íˆ ì¸ì§€í•˜ê³  ê²½ê³ í–ˆëŠ”ê°€?
        - ë°˜ë°• ê³¼ì •ì—ì„œ ìˆ¨ê²¨ì§„ ë¦¬ìŠ¤í¬ë¥¼ ë°œê²¬í–ˆëŠ”ê°€?

        4. **ì‹¤í–‰ ê°€ëŠ¥ì„± (Actionability)**
        - ì‹¤ì œ íˆ¬ìì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì  ì „ëµì¸ê°€?

        5. **ë°ì´í„° ê·¼ê±° (Evidence-Based)**
        - ì‹¤ì œ ë°ì´í„°(ìˆ˜ì¹˜)ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¸ìš©í•˜ê³  í™œìš©í–ˆëŠ”ê°€?

        ---
        **ì¶œë ¥ í˜•ì‹ (JSONë§Œ ì¶œë ¥, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ í¬í•¨ ê¸ˆì§€)**
        {{
          "system_a_debate": {{
            "logical_coherence": {{"score": 8, "reason": "í† ë¡ ì„ í†µí•´ ë…¼ë¦¬ê°€ ê²€ì¦ë¨"}},
            "multi_perspective": {{"score": 9, "reason": "ì°¨íŠ¸/ì¬ë¬´/ë‰´ìŠ¤ ê° ê´€ì ì´ í† ë¡ ìœ¼ë¡œ ì‹¬í™”"}},
            "risk_awareness": {{"score": 9, "reason": "ë°˜ë°• ê³¼ì •ì—ì„œ ë¶€ì±„ ë¦¬ìŠ¤í¬ ë°œê²¬"}},
            "actionability": {{"score": 8, "reason": "êµ¬ì²´ì  ì§„ì…ê°€/ëª©í‘œê°€ ì œì‹œ"}},
            "evidence_based": {{"score": 9, "reason": "ê° ì „ë¬¸ê°€ê°€ ìˆ˜ì¹˜ ê·¼ê±° ì œì‹œ"}}
          }},
          "system_b_single": {{
            "logical_coherence": {{"score": 7, "reason": "ì¼ê´€ì„±ì€ ìˆìœ¼ë‚˜ ê²€ì¦ ë¶€ì¡±"}},
            "multi_perspective": {{"score": 6, "reason": "ì—¬ëŸ¬ ê´€ì  ì–¸ê¸‰í–ˆìœ¼ë‚˜ ê¹Šì´ ë¶€ì¡±"}},
            "risk_awareness": {{"score": 6, "reason": "ì¼ë¶€ ë¦¬ìŠ¤í¬ ì–¸ê¸‰í–ˆìœ¼ë‚˜ ë°œê²¬ ê³¼ì • ì—†ìŒ"}},
            "actionability": {{"score": 7, "reason": "ì „ëµ ì œì‹œí–ˆìœ¼ë‚˜ ê·¼ê±° ì•½í•¨"}},
            "evidence_based": {{"score": 7, "reason": "ë°ì´í„° ì‚¬ìš©í–ˆìœ¼ë‚˜ í™œìš©ë„ ë‚®ìŒ"}}
          }},
          "winner": "system_a_debate",
          "conclusion": "ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œì´ ë°˜ë°•ì„ í†µí•´ ë¦¬ìŠ¤í¬ë¥¼ ë°œê²¬í•˜ê³ , ê° ê´€ì ì„ ì‹¬í™”ì‹œì¼°ìŒ. íŠ¹íˆ ì¬ë¬´ ì „ë¬¸ê°€ê°€ ì°¨íŠ¸ì˜ ê¸ì •ì  ì „ë§ì— ëŒ€í•´ ë¶€ì±„ ë¦¬ìŠ¤í¬ë¥¼ ê²½ê³ í•œ ì ì´ ë‹ë³´ì„."
        }}
        """
        
        response = self.judge_llm.invoke(evaluation_prompt).content
        
        # JSON ì¶”ì¶œ
        try:
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
            else:
                return {"error": "JSON íŒŒì‹± ì‹¤íŒ¨", "raw_response": response[:500]}
        except Exception as e:
            return {"error": str(e), "raw_response": response[:500]}
    
    def _generate_summary(self, stock_results: List[Dict]) -> Dict:
        """ì¢…í•© ìš”ì•½ í†µê³„"""
        
        valid_results = [r for r in stock_results if "evaluation" in r and "error" not in r["evaluation"]]
        
        if not valid_results:
            return {"error": "ìœ íš¨í•œ í‰ê°€ ê²°ê³¼ ì—†ìŒ"}
        
        multi_wins = 0
        single_wins = 0
        draws = 0
        
        multi_scores = {
            "logical_coherence": [],
            "multi_perspective": [],
            "risk_awareness": [],
            "actionability": [],
            "evidence_based": []
        }
        
        single_scores = {
            "logical_coherence": [],
            "multi_perspective": [],
            "risk_awareness": [],
            "actionability": [],
            "evidence_based": []
        }
        
        total_debate_rounds = []
        
        for result in valid_results:
            eval_data = result["evaluation"]
            winner = eval_data.get("winner", "").lower()
            
            if "debate" in winner or "multi" in winner:
                multi_wins += 1
            elif "single" in winner:
                single_wins += 1
            else:
                draws += 1
            
            # í† ë¡  ë¼ìš´ë“œ ìˆ˜ ê¸°ë¡
            multi_result = result.get("multi_agent_result", {})
            if "debate_rounds" in multi_result:
                total_debate_rounds.append(multi_result["debate_rounds"])
            
            # ì ìˆ˜ ìˆ˜ì§‘
            system_a = eval_data.get("system_a_debate", {})
            system_b = eval_data.get("system_b_single", {})
            
            for metric in multi_scores.keys():
                if metric in system_a:
                    multi_scores[metric].append(system_a[metric].get("score", 0))
                if metric in system_b:
                    single_scores[metric].append(system_b[metric].get("score", 0))
        
        # í‰ê·  ê³„ì‚°
        def avg(lst):
            return round(sum(lst) / len(lst), 2) if lst else 0
        
        return {
            "total_evaluated": len(valid_results),
            "win_rate": {
                "multi_agent_debate": multi_wins,
                "single_agent": single_wins,
                "draws": draws
            },
            "average_scores": {
                "multi_agent_debate": {k: avg(v) for k, v in multi_scores.items()},
                "single_agent": {k: avg(v) for k, v in single_scores.items()}
            },
            "total_score": {
                "multi_agent_debate": round(sum(avg(v) for v in multi_scores.values()), 2),
                "single_agent": round(sum(avg(v) for v in single_scores.values()), 2)
            },
            "average_debate_rounds": round(avg(total_debate_rounds), 1) if total_debate_rounds else 0
        }
    
    def save_results(self, results: Dict, output_dir: str = "evaluate/results"):
        """ê²°ê³¼ ì €ì¥ (JSON + Markdown)"""
        
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. JSON ì €ì¥
        json_path = f"{output_dir}/debate_comparison_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… JSON ì €ì¥: {json_path}")
        
        # 2. Markdown ìš”ì•½ ì €ì¥
        md_path = f"{output_dir}/debate_comparison_{timestamp}_summary.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_markdown_report(results))
        
        print(f"âœ… Markdown ìš”ì•½: {md_path}")
    
    def _generate_markdown_report(self, results: Dict) -> str:
        """Markdown ë¦¬í¬íŠ¸ ìƒì„±"""
        
        summary = results.get("summary", {})
        
        md = f"""# ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ vs ë‹¨ì¼ ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸

## ğŸ“‹ ë©”íƒ€ë°ì´í„°
- **í‰ê°€ ì¼ì‹œ**: {results['metadata']['test_date']}
- **í…ŒìŠ¤íŠ¸ ì¢…ëª© ìˆ˜**: {results['metadata']['total_stocks']}
- **í‰ê°€ì**: {results['metadata']['evaluator']}
- **ë©€í‹° ì—ì´ì „íŠ¸ íƒ€ì…**: {results['metadata']['multi_agent_type']}

---

## ğŸ† ì¢…í•© ê²°ê³¼

### ìŠ¹ë¥ 
- **ë©€í‹° ì—ì´ì „íŠ¸ (í† ë¡  ì‹œìŠ¤í…œ) ìŠ¹ë¦¬**: {summary['win_rate']['multi_agent_debate']}íšŒ ğŸ‰
- **ë‹¨ì¼ ì—ì´ì „íŠ¸ ìŠ¹ë¦¬**: {summary['win_rate']['single_agent']}íšŒ
- **ë¬´ìŠ¹ë¶€**: {summary['win_rate']['draws']}íšŒ

### í† ë¡  í†µê³„
- **í‰ê·  í† ë¡  ë¼ìš´ë“œ ìˆ˜**: {summary.get('average_debate_rounds', 0)}ë¼ìš´ë“œ

### í‰ê·  ì ìˆ˜ (10ì  ë§Œì )

| í‰ê°€ ê¸°ì¤€ | ë©€í‹° (í† ë¡ ) | ë‹¨ì¼ | ì°¨ì´ | í–¥ìƒë¥  |
|----------|------------|------|------|--------|
"""
        
        multi_avg = summary['average_scores']['multi_agent_debate']
        single_avg = summary['average_scores']['single_agent']
        
        for metric in multi_avg.keys():
            m_score = multi_avg[metric]
            s_score = single_avg[metric]
            diff = m_score - s_score
            diff_str = f"+{diff:.2f}" if diff > 0 else f"{diff:.2f}"
            improvement = f"+{(diff/s_score)*100:.1f}%" if s_score > 0 else "N/A"
            
            # í•µì‹¬ ì§€í‘œ ê°•ì¡°
            emoji = " â­" if metric in ["multi_perspective", "risk_awareness"] else ""
            
            md += f"| {metric}{emoji} | {m_score} | {s_score} | {diff_str} | {improvement} |\n"
        
        md += f"""
### ì´ì 
- **ë©€í‹° ì—ì´ì „íŠ¸ (í† ë¡ )**: {summary['total_score']['multi_agent_debate']}/50
- **ë‹¨ì¼ ì—ì´ì „íŠ¸**: {summary['total_score']['single_agent']}/50
- **ì°¨ì´**: +{summary['total_score']['multi_agent_debate'] - summary['total_score']['single_agent']:.2f}ì 

---

## ğŸ“Š ê°œë³„ ì¢…ëª© í‰ê°€

"""
        
        for stock in results['stocks']:
            if 'error' in stock:
                md += f"### {stock['company']}\nâŒ í‰ê°€ ì‹¤íŒ¨: {stock['error']}\n\n"
                continue
            
            eval_data = stock.get('evaluation', {})
            if 'error' in eval_data:
                md += f"### {stock['company']}\nâš ï¸ í‰ê°€ ì˜¤ë¥˜\n\n"
                continue
            
            winner = eval_data.get('winner', 'unknown')
            conclusion = eval_data.get('conclusion', '')
            
            # í† ë¡  ë¼ìš´ë“œ ì •ë³´
            multi_result = stock.get('multi_agent_result', {})
            debate_rounds = multi_result.get('debate_rounds', 0)
            
            md += f"""### {stock['company']} ({stock['ticker']})
**ìŠ¹ì**: {winner} ğŸ†
**í† ë¡  ë¼ìš´ë“œ**: {debate_rounds}íšŒ

**ê²°ë¡ **: {conclusion}

---

"""
        
        # í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
        md += f"""
## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### í† ë¡ ì˜ íš¨ê³¼
1. **ë‹¤ê°ì  ê´€ì  ì‹¬í™”**: í† ë¡ ì„ í†µí•´ ê° ì „ë¬¸ê°€ì˜ ê´€ì ì´ ë”ìš± ê¹Šì–´ì§
2. **ë¦¬ìŠ¤í¬ ë°œê²¬**: ë°˜ë°• ê³¼ì •ì—ì„œ ë‹¨ì¼ ì—ì´ì „íŠ¸ê°€ ë†“ì¹œ ë¦¬ìŠ¤í¬ ë°œê²¬
3. **ë…¼ë¦¬ ê²€ì¦**: ìƒí˜¸ ì§ˆì˜ì‘ë‹µì„ í†µí•´ ë¶„ì„ì˜ íƒ€ë‹¹ì„± ê²€ì¦

### ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤
- **ì°¨íŠ¸ ì „ë¬¸ê°€**: "ê¸°ìˆ ì ìœ¼ë¡œ ìƒìŠ¹ ì¶”ì„¸ì…ë‹ˆë‹¤"
- **ì‚¬íšŒì**: "ì¬ë¬´ ë¶„ì„ê°€ë‹˜, ì´ ìƒìŠ¹ì´ ì§€ì† ê°€ëŠ¥í• ê¹Œìš”?"
- **ì¬ë¬´ ì „ë¬¸ê°€**: "ë¶€ì±„ë¹„ìœ¨ì´ ë†’ì•„ ë¦¬ìŠ¤í¬ê°€ ìˆìŠµë‹ˆë‹¤"
- **ì°¨íŠ¸ ì „ë¬¸ê°€**: "ê·¸ë ‡ë‹¤ë©´ ë‹¨ê¸° ì „ëµìœ¼ë¡œ ìˆ˜ì •í•˜ê² ìŠµë‹ˆë‹¤"
â†’ **í† ë¡ ì„ í†µí•´ ë¦¬ìŠ¤í¬ ë°œê²¬ ë° ì „ëµ ìˆ˜ì •!**

"""
        
        return md


# ============================================================
# ì‹¤í–‰
# ============================================================

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í…ŒìŠ¤íŠ¸í•  ì¢…ëª© (í•œê¸€ ê¸°ì—…ëª…)
    test_stocks = [
        "ì‚¼ì„±ì „ì",
        # "SKí•˜ì´ë‹‰ìŠ¤",  # ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
        # "ì¹´ì¹´ì˜¤"
    ]
    
    print("\n" + "="*70)
    print("  ğŸ­ ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ vs ë‹¨ì¼ ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¹„êµ")
    print("="*70)
    print(f"\nâ±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {len(test_stocks) * 3}ë¶„")
    print("ğŸ’¡ ë©€í‹° ì—ì´ì „íŠ¸ëŠ” ì‹¤ì œ í† ë¡  ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (ê¸°ì¡°ë°œì–¸ + í† ë¡  + ìµœí›„ë³€ë¡  + íŒì •)")
    print("\n")
    
    comparator = DebateComparator()
    
    # ë¹„êµ ì‹¤í–‰
    results = await comparator.run_comparison(test_stocks)
    
    # ê²°ê³¼ ì €ì¥
    comparator.save_results(results)
    
    # ì½˜ì†” ì¶œë ¥
    print("\n" + "="*70)
    print("  ğŸ“Š ìµœì¢… ìš”ì•½")
    print("="*70 + "\n")
    
    summary = results.get("summary", {})
    
    if "error" in summary:
        print(f"âŒ ì˜¤ë¥˜: {summary['error']}")
        return
    
    print(f"ì´ í‰ê°€ ì¢…ëª©: {summary.get('total_evaluated', 0)}ê°œ")
    print(f"í‰ê·  í† ë¡  ë¼ìš´ë“œ: {summary.get('average_debate_rounds', 0)}íšŒ")
    
    print(f"\nğŸ† ìŠ¹ë¥ :")
    print(f"  - ë©€í‹° ì—ì´ì „íŠ¸ (í† ë¡ ): {summary['win_rate']['multi_agent_debate']}ìŠ¹")
    print(f"  - ë‹¨ì¼ ì—ì´ì „íŠ¸: {summary['win_rate']['single_agent']}ìŠ¹")
    print(f"  - ë¬´ìŠ¹ë¶€: {summary['win_rate']['draws']}íšŒ")
    
    print(f"\nğŸ“ˆ í‰ê·  ì ìˆ˜:")
    print(f"  - ë©€í‹° ì—ì´ì „íŠ¸ (í† ë¡ ): {summary['total_score']['multi_agent_debate']}/50")
    print(f"  - ë‹¨ì¼ ì—ì´ì „íŠ¸: {summary['total_score']['single_agent']}/50")
    
    # í•µì‹¬ ì§€í‘œ ê°•ì¡°
    multi_scores = summary['average_scores']['multi_agent_debate']
    single_scores = summary['average_scores']['single_agent']
    
    print(f"\nâ­ í•µì‹¬ ì§€í‘œ ë¹„êµ:")
    print(f"  - ë‹¤ê°ì  ê´€ì : {multi_scores['multi_perspective']} vs {single_scores['multi_perspective']}")
    improvement = ((multi_scores['multi_perspective'] - single_scores['multi_perspective']) / single_scores['multi_perspective']) * 100
    print(f"    â†’ ë©€í‹°ê°€ {improvement:.1f}% ë” ìš°ìˆ˜!")
    
    print(f"  - ë¦¬ìŠ¤í¬ ì¸ì‹: {multi_scores['risk_awareness']} vs {single_scores['risk_awareness']}")
    improvement = ((multi_scores['risk_awareness'] - single_scores['risk_awareness']) / single_scores['risk_awareness']) * 100
    print(f"    â†’ ë©€í‹°ê°€ {improvement:.1f}% ë” ìš°ìˆ˜!")
    
    print("\n" + "="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())